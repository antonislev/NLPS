**âœï¸AUTHOR**
**Î›Î•Î’Î•Î™Î”Î™Î©Î¤Î—Î£ Î‘ÎÎ¤Î©ÎÎ—Î£** (Î‘Îœ: Î 22084)
Î¤Î¼Î®Î¼Î± Î Î»Î·ÏÎ¿Ï†Î¿ÏÎ¹ÎºÎ®Ï‚, Î Î±Î½ÎµÏ€Î¹ÏƒÏ„Î®Î¼Î¹Î¿ Î ÎµÎ¹ÏÎ±Î¹ÏÏ‚
ÎˆÏ„Î¿Ï‚: 2025

---

## ğŸ“Œ Î ÎµÏÎ¯Î»Î·ÏˆÎ·

Î— ÎµÏÎ³Î±ÏƒÎ¯Î± Î±Ï…Ï„Î® Ï€Î±ÏÎ¿Ï…ÏƒÎ¹Î¬Î¶ÎµÎ¹ Î­Î½Î± Î¿Î»Î¿ÎºÎ»Î·ÏÏ‰Î¼Î­Î½Î¿ Ï€Î»Î±Î¯ÏƒÎ¹Î¿ **ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ®Ï‚ Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î®Ï‚ ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…**. Î£Ï…Î½Î´Ï…Î¬Î¶Î¿Î½Ï„Î±Î¹ ruleâ€‘based Î±Ï…Ï„Î¿Î¼Î±Ï„Î¿Ï€Î¿Î¹Î·Î¼Î­Î½Î¿Î¹ ÎºÎ±Î½ÏŒÎ½ÎµÏ‚ (DFA), Ï€Î±ÏÎ±Î´Î¿ÏƒÎ¹Î±ÎºÎ¬ NLP pipelines (spaCy, NLTK, Gensim) ÎºÎ±Î¹ ÏƒÏÎ³Ï‡ÏÎ¿Î½ÎµÏ‚ ÎµÎ½ÏƒÏ‰Î¼Î±Ï„ÏÏƒÎµÎ¹Ï‚ Î»Î­Î¾ÎµÏ‰Î½ (Word2Vec, GloVe, FastText, BERT). Î ÏÎ±Î³Î¼Î±Ï„Î¿Ï€Î¿Î¹ÎµÎ¯Ï„Î±Î¹ ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ· Ï‰Ï‚ Ï€ÏÎ¿Ï‚ Î»ÎµÎ¾Î¹Î»Î¿Î³Î¹ÎºÎ® Ï€Î¹ÏƒÏ„ÏŒÏ„Î·Ï„Î± (TTR, Jaccard), ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ® ÏƒÏ…Î½Î¬Ï†ÎµÎ¹Î± (cosine similarity) ÎºÎ±Î¹ Î¿Ï€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· Î¼ÎµÏ„Î±Ï„Î¿Ï€Î¯ÏƒÎµÏ‰Î½ Î¼Î­ÏƒÏ‰ PCA/tâ€‘SNE. Î¤Î± Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î± ÎµÏ€Î¹Î²ÎµÎ²Î±Î¹ÏÎ½Î¿Ï…Î½ Ï„Î· Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· Î½Î¿Î®Î¼Î±Ï„Î¿Ï‚ ÎºÎ±Î¹ Î±Î½Î±Î´ÎµÎ¹ÎºÎ½ÏÎ¿Ï…Î½ tradeâ€‘offs Î¼ÎµÏ„Î±Î¾Ï ÎºÎ±Î¸Î±ÏÏŒÏ„Î·Ï„Î±Ï‚ ÎºÎ±Î¹ Î³ÎµÎ½Î¯ÎºÎµÏ…ÏƒÎ·Ï‚.

---

## 1. Î•Î¹ÏƒÎ±Î³Ï‰Î³Î®

Î— **ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ® Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î® ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…** ÏƒÏ„Î¿Ï‡ÎµÏÎµÎ¹ ÏƒÏ„Î·Î½ Ï€Î±ÏÎ±Î³Ï‰Î³Î® ÎºÎ±Î¸Î±ÏÏÎ½, ÏƒÏ…Î½ÎµÎºÏ„Î¹ÎºÏÎ½ ÎºÎ±Î¹ Î±ÎºÏÎ¹Î²ÏÎ½ ÎµÎºÎ´ÏŒÏƒÎµÏ‰Î½ Ï€ÏÏ‰Ï„Î¿Î³ÎµÎ½ÏÎ½ ÎºÎµÎ¹Î¼Î­Î½Ï‰Î½, Î´Î¹Î±Ï„Î·ÏÏÎ½Ï„Î±Ï‚ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± Ï„Î¿ Î±ÏÏ‡Î¹ÎºÏŒ Î½ÏŒÎ·Î¼Î±. ÎœÎ­ÏƒÏ‰ Ï„ÎµÏ‡Î½Î¹ÎºÏÎ½ NLP Î±Ï†Î±Î¹ÏÎ¿ÏÎ¼Îµ Î¸ÏŒÏÏ…Î²Î¿, Î³ÏÎ±Î¼Î¼Î±Ï„Î¹ÎºÎ­Ï‚ Î±Î½Ï‰Î¼Î±Î»Î¯ÎµÏ‚ ÎºÎ±Î¹ Ï€Î»ÎµÎ¿Î½Î±ÏƒÎ¼Î¿ÏÏ‚, ÎµÎ½Ï Î¼Îµ embeddings Î±Ï€Î¿Ï„Ï…Ï€ÏÎ½Î¿Ï…Î¼Îµ ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÎ­Ï‚ ÏƒÏ‡Î­ÏƒÎµÎ¹Ï‚.

**Î£Ï…Î¼Î²Î¿Î»Î® ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚**:

1. ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ ÎºÎ±Î¹ Ï…Î»Î¿Ï€Î¿Î¯Î·ÏƒÎ· **DFA** Î³Î¹Î± Î´ÏÎ¿ Ï€Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚ (DFAâ€‘based reconstruction).
2. Î£ÏÎ³ÎºÏÎ¹ÏƒÎ· **Ï„ÏÎ¹ÏÎ½ Python pipelines** (spaCy, NLTK, Gensim) Î³Î¹Î± Î¿Î»ÏŒÎºÎ»Î·ÏÎ± ÎºÎµÎ¯Î¼ÎµÎ½Î±.
3. Î•Ï†Î±ÏÎ¼Î¿Î³Î® **Word2Vec, GloVe, FastText, BERT embeddings** ÎºÎ±Î¹ custom flows (Whitespace, Regex, Stopwords) Î¼Îµ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ cosine similarity.
4. ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· ÏƒÎ·Î¼Î±ÏƒÎ¹Î¿Î»Î¿Î³Î¹ÎºÏÎ½ Î¼ÎµÏ„Î±Ï„Î¿Ï€Î¯ÏƒÎµÏ‰Î½ Î¼Î­ÏƒÏ‰ **PCA** ÎºÎ±Î¹ **tâ€‘SNE**.

---

## 2. ÎœÎµÎ¸Î¿Î´Î¿Î»Î¿Î³Î¯Î±

### 2.1. Î‘Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î® Î´ÏÎ¿ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÏ‰Î½ (DFA)

Î§ÏÎ·ÏƒÎ¹Î¼Î¿Ï€Î¿Î¹Î®Î¸Î·ÎºÎµ Î­Î½Î±Ï‚ **Deterministic Finite Automaton** Î³Î¹Î± Ï„Î¹Ï‚ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚:

* **A**: "The quick brown fox"
* **B**: "Jump over lazy dog"

**ÎŸÏÎ¹ÏƒÎ¼ÏŒÏ‚ DFA**:

* ÎšÎ±Ï„Î±ÏƒÏ„Î¬ÏƒÎµÎ¹Ï‚: `q0` (start), `q1`â€“`q4` (A), `q5`â€“`q8` (B).
* Î‘Î»Ï†Î¬Î²Î·Ï„Î¿: Î¿Î¹ Î»Î­Î¾ÎµÎ¹Ï‚ ÎºÎ¬Î¸Îµ Ï€ÏÏŒÏ„Î±ÏƒÎ·Ï‚.
* Accept states: `q4` (Ï„Î­Î»Î¿Ï‚ A), `q8` (Ï„Î­Î»Î¿Ï‚ B).

```python
transitions = {
    ("q0", "The"): "q1",
    ("q1", "quick"): "q2",
    ("q2", "brown"): "q3",
    ("q3", "fox"): "q4",
    ("q0", "Jump"): "q5",
    ("q5", "over"): "q6",
    ("q6", "lazy"): "q7",
    ("q7", "dog"): "q8"
}
```

ÎœÎ­ÏƒÏ‰ **BFS** ÎµÎ¾Î¬Î³Î¿Î½Ï„Î±Î¹ Î¿Î¹ Î¼Î¿Î½Î±Î´Î¹ÎºÎ­Ï‚ Î´Î¹Î±Î´ÏÎ¿Î¼Î­Ï‚ Ï€ÏÎ¿Ï‚ `q4` ÎºÎ±Î¹ `q8`, Î±Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î¬Î¶Î¿Î½Ï„Î±Ï‚ Î±ÎºÏÎ¹Î²ÏÏ‚ Ï„Î¹Ï‚ Î´ÏÎ¿ Ï€ÏÎ¿Ï„Î¬ÏƒÎµÎ¹Ï‚.

### 2.2. Î¤ÏÎ¯Î± Python pipelines Î³Î¹Î± Î¿Î»ÏŒÎºÎ»Î·ÏÎ± ÎºÎµÎ¯Î¼ÎµÎ½Î±

Î•Ï†Î±ÏÎ¼ÏŒÏƒÏ„Î·ÎºÎ±Î½ ÏƒÏ„Î± Î´ÏÎ¿ Ï€ÏÏ‰Ï„Î¿Î³ÎµÎ½Î® ÎºÎµÎ¯Î¼ÎµÎ½Î± (Text1, Text2) Î¿Î¹ ÎµÎ¾Î®Ï‚ ÏÎ¿Î­Ï‚:

1. **spaCy Lemmatization**: tokenization â†’ Î±Ï†Î±Î¯ÏÎµÏƒÎ· stopwords â†’ Î»ÎµÎ¼Î¼Î±Ï„Î¿Ï€Î¿Î¯Î·ÏƒÎ·.
2. **NLTK RegexpTokenizer**: regex-based tokenization â†’ stopwords â†’ WordNet lemmatizer.
3. **Gensim simple\_preprocess**: deaccented tokenization â†’ Gensim stopword list.

ÎšÎ±Ï„ÏŒÏ€Î¹Î½ ÎµÏ€Î±Î½Î±ÏƒÏ…Î½Ï„Î­Î¸Î·ÎºÎ±Î½ Ï„Î± tokens ÏƒÎµ ÏƒÏ…Î½ÎµÏ‡Î® ÎºÎµÎ¯Î¼ÎµÎ½Î± Î³Î¹Î± Ï€ÎµÏÎ±Î¹Ï„Î­ÏÏ‰ Î±Î½Î¬Î»Ï…ÏƒÎ·.

### 2.3. Metrics ÏƒÏÎ³ÎºÏÎ¹ÏƒÎ·Ï‚

Î“Î¹Î± ÎºÎ¬Î¸Îµ pipeline ÎºÎ±Î¹ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Ï…Ï€Î¿Î»Î¿Î³Î¯ÏƒÏ„Î·ÎºÎ±Î½:

* **Total tokens** & **Unique tokens**
* **Typeâ€“Token Ratio (TTR)** = Unique / Total
* **Jaccard Similarity** vs. Î±ÏÏ‡Î¹ÎºÏŒ Î»ÎµÎ¾Î¹Î»ÏŒÎ³Î¹Î¿

Î¤Î± metrics Î¿Î¼Î±Î´Î¿Ï€Î¿Î¹Î®Î¸Î·ÎºÎ±Î½ ÏƒÎµ DataFrame ÎºÎ±Î¹ Ï€Î±ÏÎ¿Ï…ÏƒÎ¹Î¬ÏƒÏ„Î·ÎºÎ±Î½ Î¼Îµ Î³ÏÎ±Î¼Î¼Î¹ÎºÎ¬ Î´Î¹Î±Î³ÏÎ¬Î¼Î¼Î±Ï„Î±.

---

## 3. Î•Î½ÏƒÏ‰Î¼Î±Ï„ÏÏƒÎµÎ¹Ï‚ Î›Î­Î¾ÎµÏ‰Î½ & Semantic Analysis

### 3.1. Î ÏÎ¿ÎµÎºÏ€Î±Î¹Î´ÎµÏ…Î¼Î­Î½Î± models

* **Word2Vec** (Google News, 300d)
* **GloVe** (Wikiâ€‘Gigaword, 50d)
* **FastText** (Wikiâ€‘Subwords, 300d)
* **BERT** (bert-base-uncased, mean-pooled)

### 3.2. Î¥Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒÏ‚ cosine similarity

Î“Î¹Î± ÎºÎ¬Î¸Îµ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ (Ï€ÏÎ¹Î½/Î¼ÎµÏ„Î¬) Ï…Ï€Î¿Î»Î¿Î³Î¯ÏƒÏ„Î·ÎºÎµ Ï„Î¿ Î¼Î­ÏƒÎ¿ embedding vector ÎºÎ±Î¹ Î· **cosine similarity**:

```python
vec_orig = mean_embedding(orig_tokens, model)
vec_recon = mean_embedding(recon_tokens, model)
score = 1 - cosine(vec_orig, vec_recon)
```

### 3.3. Custom NLP flows

Î•Ï€Î¹Ï€Î»Î­Î¿Î½ examiner pipelines:

* Whitespace Split
* Regex Tokenize
* Remove Stopwords
* Lemmatize (spaCy)
* WordNet + Hypernyms

### 3.4. ÎŸÏ€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· (PCA & tâ€‘SNE)

Î‘ÏÏ‡Î¹ÎºÎ¬ ÎµÏ†Î±ÏÎ¼ÏŒÏƒÏ„Î·ÎºÎµ **PCAâ†’2D**, ÏƒÏ„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î± **tâ€‘SNEâ†’2D** (perplexity=30, init='random').

---

## 4. Î ÎµÎ¹ÏÎ¬Î¼Î±Ï„Î± & Î‘Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±

### 4.1. Î‘Î½Î±ÎºÎ±Ï„Î±ÏƒÎºÎµÏ…Î­Ï‚

**Î Î±ÏÎ¬Î´ÎµÎ¹Î³Î¼Î± A**:

* Î ÏÎ¹Î½: "Today is our dragon boat festival"
* DFA:  "Today is our dragon boat festival"

**Text1 (spaCy)**:

* Î ÏÎ¹Î½: Today is our dragon boat festival, in our Chinese culture, to celebrate...
* ÎœÎµÏ„Î¬: today dragon boat festival chinese culture celebrate safe great lives hope

### 4.2. Metrics

| Pipeline | Text  | Total | Unique | TTR   | Jaccard |
| -------- | ----- | ----- | ------ | ----- | ------- |
| spaCy    | Text1 | 37    | 33     | 0.892 | 0.524   |
| NLTK     | Text2 | 59    | 57     | 0.966 | 0.525   |
| Gensim   | Text1 | 37    | 33     | 0.892 | 0.524   |

### 4.3. Cosine Similarities

| Model    | Text1 vs Recon | Text2 vs Recon |
| -------- | -------------- | -------------- |
| Word2Vec | 0.985          | 0.978          |
| GloVe    | 0.982          | 0.975          |
| FastText | 0.987          | 0.980          |
| BERT     | 0.991          | 0.989          |

---

## 5. Î£Ï…Î¶Î®Ï„Î·ÏƒÎ·

ÎŸÎ¹ Ï…ÏˆÎ·Î»Î­Ï‚ similarity scores (>0.97) ÎµÏ€Î¹Î²ÎµÎ²Î±Î¹ÏÎ½Î¿Ï…Î½ Ï„Î· Î´Î¹Î±Ï„Î®ÏÎ·ÏƒÎ· Ï„Î¿Ï… Î½Î¿Î®Î¼Î±Ï„Î¿Ï‚ Î±ÎºÏŒÎ¼Î· ÎºÎ±Î¹ Î¼ÎµÏ„Î¬ Î±Ï€ÏŒ Î­Î½Ï„Î¿Î½Î¿ preprocessing.
ÎŸÎ¹ pipelines Î¼Îµ stopword removal ÎºÎ±Î¹ hypernyms Ï€Î±ÏÎ¿Ï…ÏƒÎ¯Î±ÏƒÎ±Î½ Î¼Î¹ÎºÏÏŒÏ„ÎµÏÎµÏ‚ Ï„Î¹Î¼Î­Ï‚ (\~0.90â€“0.92) ÎµÎ¾Î±Î¹Ï„Î¯Î±Ï‚ Î»ÎµÎ¾Î¹Î»Î¿Î³Î¹ÎºÏÎ½ Î±Ï€Ï‰Î»ÎµÎ¹ÏÎ½.
Î— Î¿Ï€Ï„Î¹ÎºÎ¿Ï€Î¿Î¯Î·ÏƒÎ· PCA/tâ€‘SNE Î´ÎµÎ¯Ï‡Î½ÎµÎ¹ ÏƒÏ„Î±Î¸ÎµÏÎ­Ï‚ Ï„Î¿Ï€Î¹ÎºÎ­Ï‚ Î´Î¿Î¼Î­Ï‚: Î¿Î¹ Î»Î­Î¾ÎµÎ¹Ï‚ Î´Î¹Î±Ï„Î·ÏÎ¿ÏÎ½ Ï„Î¹Ï‚ Î¿Î¼Î¬Î´ÎµÏ‚ Ï„Î¿Ï…Ï‚ (Ï€.Ï‡. Î¶ÏÎ±, Ï‡ÏÏÎ¼Î±Ï„Î±).

---

## 6. Î£Ï…Î¼Ï€ÎµÏÎ¬ÏƒÎ¼Î±Ï„Î± & ÎœÎµÎ»Î»Î¿Î½Ï„Î¹ÎºÎ­Ï‚ Î•ÏÎ³Î±ÏƒÎ¯ÎµÏ‚

* Î”ÎµÎ½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î¼Î¯Î± Î¹Î´Î±Î½Î¹ÎºÎ® pipelineÂ· Î· ÎµÏ€Î¹Î»Î¿Î³Î® ÎµÎ¾Î±ÏÏ„Î¬Ï„Î±Î¹ Î±Ï€ÏŒ Ï„Î¿Î½ ÏƒÏ„ÏŒÏ‡Î¿: **Ï€Î¹ÏƒÏ„ÏŒÏ„Î·Ï„Î±** vs. **Î³ÎµÎ½Î¯ÎºÎµÏ…ÏƒÎ·**.
* Î£Ï…Î½Î´Ï…Î±ÏƒÏ„Î¹ÎºÎ¬ ruleâ€‘based + transformer pipelines Î¼Ï€Î¿ÏÎ¿ÏÎ½ Î½Î± ÎµÎ½Î¹ÏƒÏ‡ÏÏƒÎ¿Ï…Î½ contextual paraphrasing.
* ÎœÎµÎ»Î»Î¿Î½Ï„Î¹ÎºÎ¬: fineâ€‘tuned seq2seq models (BART, T5) ÏƒÎµ Ï€Î±ÏÎ¬Î»Î»Î·Î»Î± corpus Î³Î¹Î± endâ€‘toâ€‘end Ï€Î±ÏÎ±Ï†ÏÎ¬ÏƒÎµÎ¹Ï‚.

---

## ğŸ“ Requirements

```text
numpy>=1.18.5
scipy>=1.7.0
gensim>=4.3.3
scikit-learn>=1.2
matplotlib>=3.5
torch>=1.13
transformers>=4.30
nltk
seaborn
pandas
```
