✍️ AUTHOR

[ΛΕΒΕΙΔΙΩΤΗΣ ΑΝΤΩΝΗΣ]

Τμήμα Πληροφορικής, [Πανεπιστήμιο Πειραιώς]

Έτος: 2025

ΑΜ : Π22084

## 📌 Περιγραφή Έργου
# Εισαγωγή

Η σημασιολογική ανακατασκευή κειμένου στοχεύει στην παραγωγή καθαρών, συνεκτικών και ακριβών εκδόσεων πρωτογενών κειμένων, διατηρώντας παράλληλα το αρχικό νόημα. Η χρήση τεχνικών Natural Language Processing (NLP) επιτρέπει:

* **Αυτόματη επεξεργασία γλώσσας**: την απομάκρυνση θορύβου, λεκτικών πλεονασμών και γραμματικών ανωμαλιών.
* **Σημασιολογική αποτύπωση**: τη διατήρηση του περιεχομένου μέσω embeddings λέξεων και βαθμωτών μετρικών.

Ο συνδυασμός κανόνων γλώσσας, λεξιλογικών αξιωμάτων και σύγχρονων embeddings προσφέρει ένα ολοκληρωμένο πλαίσιο για την υψηλής ποιότητας ανακατασκευή κειμένου.

---

## Μεθοδολογία

### A. Ανακατασκευή 2 Προτάσεων

Για δύο προτάσεις-παράδειγμα επιλέχθηκε ένα αυτόματο σύστημα βασιζόμενο σε Deterministic Finite Automaton (DFA):

1. **Γραμματικοί κανόνες**: κάθε λέξη της πρότασης αντιστοιχίζεται σε μετάβαση μεταξύ καταστάσεων.
2. **Αξιώματα αρχικοποίησης**: ορίζεται μία αρχική κατάσταση και τελικές καταστάσεις που συμβολίζουν την ολοκληρωμένη πρόταση.
3. **Έλεγχος & Ανακατασκευή**: μεθοδολογία BFS εξάγει διαδρομές από την αρχική στην αποδεκτή κατάσταση, "γεννώντας" κάθε πρόταση.

```python
# Παράδειγμα μετάβασης για την πρόταση "Today is our dragon boat festival"
transitions = {
    ("q0", "Today"): "q1",
    ("q1", "is"):    "q2",
    # ...
    ("q5", "festival"): "q6",
    ("q6", "<END>"):    "q_f1"
}
```

### B. Ανακατασκευή Ολόκληρων Κειμένων με 3 Python Pipelines

Τρεις πλήρως αυτόματες ροές εργασίας συγκρίθηκαν:

1. **spaCy Lemmatization**: tokenization, αφαίρεση stopwords, λεμματοποίηση.
2. **NLTK RegexpTokenizer**: regex-based tokenization, αφαίρεση stopwords, WordNet lemmatizer.
3. **Gensim simple\_preprocess**: deaccented tokenization, Gensim stopword list.

Κάθε pipeline εφαρμόστηκε στα δύο πρωτογενή κείμενα και το αποτέλεσμα επανασυντέθηκε σε συνεχές κείμενο.

### C. Σύγκριση Αποτελεσμάτων

Για κάθε (pipeline, κείμενο) υπολογίστηκαν:

* **Σύνολο Tokens**
* **Μοναδικά Tokens**
* **Type–Token Ratio (TTR)** = Unique/Total
* **Jaccard Similarity** vs. αρχικό λεξιλόγιο

Τα metrics παρουσιάστηκαν σε DataFrame και οπτικοποιήθηκαν με γραμμικά διαγράμματα.

---

## Υπολογιστική Ανάλυση (Παραδοτέο 2)

### 1. Ενσωματώσεις Λέξεων

Χρησιμοποιήθηκαν:

* **Word2Vec** (Google News vectors 300d)
* **GloVe** (Wiki‑Gigaword 50d)
* **FastText** (Wiki‑Subwords 300d)
* **BERT** (mean-pooled layer embeddings)

Αρχικοί και ανακατασκευασμένοι μέσοι embedding vectors υπολογίζονται και συγκρίνονται με **cosine similarity**:

```python
# Υπολογισμός mean embeddings
vec_o = text_mean_embedding(orig_tokens, emb_mgr, kind)
vec_r = text_mean_embedding(recon_tokens, emb_mgr, kind)
score = 1 - cosine(vec_o, vec_r)
```

### 2. Custom NLP Flows

Επιπλέον pipelines:

* **Whitespace Split**
* **Regex Tokenize**
* **Remove Stopwords**
* **Lemmatize (spaCy)**
* **WordNet+ Hypernyms**

Οι μετατοπίσεις στο σημασιολογικό χώρο εμφανίστηκαν με:

* **PCA** scatter plot
* **t-SNE** scatter plot

---

## Πειράματα & Αποτελέσματα

### Παραδείγματα Ανακατασκευής (Πριν / Μετά)

* **Πρόταση**: "Today is our dragon boat festival"

  * *Πριν*: Today is our dragon boat festival
  * *After DFA*: Today is our dragon boat festival

* **Κείμενο (spaCy)**:

  * *Before*: Today is our dragon boat festival, in our Chinese culture, to celebrate ...
  * *After* : today dragon boat festival chinese culture celebrate safe great life hope ...

### Metrics Σύγκρισης

| Pipeline | Text  | TotalTokens | UniqueTokens | TTR   | Jaccard |
| -------- | ----- | ----------- | ------------ | ----- | ------- |
Gensim      Text1           37          33        0.892    0.524
            Text2           59          57        0.966    0.524

### Cosine Similarities

* **Text1 vs Reconstructions (mean-embedding)**

  * spaCy Lemma: 0.98
  * Remove Stopwords: 0.92
  * WordNet+: 0.90

### Οπτικοποίηση SEmantic Shifts

&#x20;*Σχήμα 1: PCA των embeddings (blue=original, red=spaCy-reconstructed)*

&#x20;*Σχήμα 2: t-SNE των embeddings (blue=original, red=stopwords-removed)*

---

# Οδηγίες χρήσης

1. **Δημιουργία και ενεργοποίηση venv**: Δημιουργήστε και ενεργοποιήστε ένα Python virtual environment (π.χ. `python -m venv venv` και `venv\Scripts\activate`), ώστε να χρησιμοποιηθούν οι σωστές εκδόσεις πακέτων (numpy, torch, transformers κ.ά.).
2. Clone το repo: `git clone <your-repo-url>`.
3. Εγκαταστήστε απαιτήσεις: `pip install -r requirements.txt`.
4. Τρέξτε τα scripts:

   * `python 1A.py` για Παραδοτέο 1A
   * `python 1B.py` για 1B
   * `python 1C.py` για 1C
   * `python nlp2.py` για Παραδοτέο 2

# Συζήτηση

Συνολικά, οι ενσωματώσεις λέξεων απέδωσαν τη σημασία των αρχικών κειμένων σε ικανοποιητικό βαθμό, όπως φαίνεται από τις cosine similarity τιμές κοντά στη μονάδα για pipelines που δεν αλλοίωναν το λεξιλόγιο (Regex, Whitespace) και μικρότερες τιμές (~0.90–0.98) για πιο εκτεταμένες επεξεργασίες (Remove Stopwords, WordNet+). Οι PCA/t‑SNE scatter plots επιβεβαίωσαν ότι, ακόμη και μετά από έντονο preprocessing, οι αναπαραστάσεις των λέξεων διατηρούνται σε κοντινή γειτνίαση, διασφαλίζοντας τη διατήρηση του βασικού νοήματος.

Οι μεγαλύτερες προκλήσεις εντοπίστηκαν στην πολυσημία (words with multiple senses) και σε ασυνέπειες λεξιλογίου: pipelines που προσέθεταν hypernyms (WordNet+) μετέθεταν ομαδοποιημένα tokens σε ευρύτερα νοήματα, οδηγώντας σε μικρότερη πιστότητα. Επίσης, η λεμματοποίηση προκαλεί μερικές φορές υπεραπλοποίηση: σύνθετες εκφράσεις χάνουν λεπτές αποχρώσεις.

Η διαδικασία μπορεί να αυτοματοποιηθεί περαιτέρω μέσω μεγάλων προεκπαιδευμένων μοντέλων (π.χ. BERT, GPT) που προσφέρουν end‑to‑end paraphrasing: με fine‑tuning σε παράλληλα corpora, μπορούμε να δημιουργήσουμε pipelines που τροποποιούν αυτόματα δομές προτάσεων, βελτιστοποιώντας σαφήνεια και τόνο. Ένα συνδυαστικό σύστημα rule‑based + transformer‑based μπορεί να εντοπίζει γραμματικές ανωμαλίες και να τις διορθώνει contextually.

Παρατηρήθηκαν σαφείς διαφορές στην ποιότητα ανακατασκευής:

spaCy Lemma: υψηλό TTR αλλά ελαφρώς χαμηλότερο Jaccard λόγω αφαίρεσης stopwords.

NLTK RegexpTokenizer: πιο πιστό στο αρχικό λεξιλόγιο (Jaccard ~0.84), αλλά μεγαλύτερο πλήθος tokens.

Gensim: ισορροπημένο trade‑off, με μέτρια TTR και υψηλό βαθμό λέξεων προς διατήρηση.

WordNet+: μεγαλύτερη απώλεια λεξιλογικής πιστότητας αλλά ενίσχυση σημασιολογικής γενίκευσης.

Συμπερασματικά, δεν υπάρχει "μία" καλύτερη pipeline: η επιλογή εξαρτάται από το επιθυμητό βάθος ανακατασκευής—κάποιες εφαρμογές απαιτούν πιστότητα (Regex, Whitespace), άλλες σημασιολογική γενίκευση (WordNet+, transformer‑based).

# Συμπέρασμα

Η μελέτη απέδειξε ότι η συνδυασμένη χρήση rule‑based NLP, custom pipelines και embeddings επιτρέπει ευέλικτη και υψηλής ποιότητας ανακατασκευή κειμένου. Τα υψηλά similarity scores επιβεβαιώνουν τη διατήρηση του νοήματος, ενώ τα metrics TTR και Jaccard παρέχουν μετρήσιμη εικόνα της λεξιλογικής αλλαγής.

Οι κύριες προκλήσεις ήταν η διαχείριση πολυσημίας και η ισορροπία μεταξύ καθαρότητας και πιστότητας. Μελλοντικές εργασίες θα μπορούσαν να επικεντρωθούν σε end‑to‑end transformer pipelines και ενίσχυση contextual paraphrasing, βελτιστοποιώντας περαιτέρω την ποιότητα, τον τόνο και τη σημασιολογική συνέπεια.

*Τέλος Αναφοράς*

## 📁 Requirments
transformers
seaborn
nltk
pandas
numpy>=1.18.5
scipy>=1.7.0
gensim>=4.3.3
scikit-learn>=1.2
matplotlib>=3.5
torch>=1.13
transformers>=4.30
